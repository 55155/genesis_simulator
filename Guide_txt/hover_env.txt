üöÅ Training Drone Hovering Policies with RL # Genesis supports parallel simulation, making it ideal for training reinforcement learning (RL) drone hovering policies efficiently. In this tutorial, we will walk you through a complete training example for obtaining a basic hovering policy that enables a drone to maintain a stable hover position by reaching randomly generated target points. This is a simple and minimal example that demonstrates a very basic RL training pipeline in Genesis, and with the following example you will be able to obtain a drone hovering policy that‚Äôs deployable to a real drone very quickly. Note : This is NOT a comprehensive drone hovering policy training pipeline. It uses simplified reward terms to get you started easily, and does not exploit Genesis‚Äôs speed on big batch sizes, so it only serves basic demonstration purposes. Acknowledgement : This tutorial is inspired by Champion-level drone racing using deep reinforcement learning (Nature 2023) . Environment Overview # We start by creating a gym-style environment (hover-env). Initialize # The __init__ function sets up the simulation environment with the following steps: Control Frequency . The simulation runs at 100 Hz, providing a high-frequency control loop for the drone. Scene Creation . A simulation scene is created, including the drone and a static plane. Target Initialization . A random target point is initialized, which the drone will attempt to reach. Reward Registration . Reward functions, defined in the configuration, are registered to guide the policy. These functions will be explained in the ‚ÄúReward‚Äù section. Buffer Initialization . Buffers are initialized to store environment states, observations, and rewards. Reset # The reset_idx function resets the initial pose and state buffers of the specified environments. This ensures robots start from predefined configurations, crucial for consistent training. Step # The step function updates the environment state based on the actions taken by the policy. It includes the following steps: Action Execution . The input action will be clipped to a valid range, rescaled, and applied as adjustments to the default hover propeller RPMs. State Update . Drone states, such as position, attitude, and velocities, are retrieved and stored in buffers. Termination Checks . Terminated environments are reset automatically. Environments are terminated if Episode length exceeds the maximum allowed. The drone‚Äôs pitch or roll angle exceeds a specified threshold. The drone‚Äôs position exceeds specified boundaries. The drone is too close to the ground. Reward Computation . Rewards are calculated based on the drone‚Äôs performance in reaching the target point and maintaining stability. Observation Computation . Observations are normalized and returned to the policy. Observations used for training include drone‚Äôs position, attitude (quaternion), body linear velocity, body angular velocity and previous actions. Reward # In this example, we use the following reward functions to encourage the drone to reach the target point and maintain stability: target : Encourages the drone to reach the randomly generated target points. smooth : Encourages smooth actions and bridge the sim-to-real gap. yaw : Encourages the drone to maintain a stable hover yaw. angular : Encourages the drone to maintain low angular velocities. crash : Penalizes the drone for crashing or deviating too far from the target. These reward functions are combined to provide comprehensive feedback to the policy, guiding it to achieve stable and accurate hovering behavior. Training # At this stage, we have defined the environments. To train the drone hovering policy using PPO, follow these steps: Install Dependencies . Ensure you have installed all necessary dependencies, including Genesis and rsl_rl . # Install rsl_rl. git clone https://github.com/leggedrobotics/rsl_rl cd rsl_rl && git checkout v1.0.2 && pip install -e . # Install tensorboard. pip install tensorboard Run Training Script . Use the provided training script to start training the policy. python hover_train.py -e drone-hovering -B 8192 --max_iterations 500 -e drone-hovering : Specifies the experiment name as ‚Äúdrone-hovering‚Äù. -B 8192 : Sets the number of environments to 8192 for parallel training. --max_iterations 500 : Specifies the maximum number of training iterations to 500. To monitor the training process, launch TensorBoard: tensorboard --logdir logs You should see a training curve similar to this: Evaluation # To evaluate the trained drone hovering policy, follow these steps: Run Evaluation Script . Use the provided evaluation script to evaluate the trained policy. python hover_eval.py -e drone-hovering --ckpt 500 --record -e drone-hovering : Specifies the experiment name as ‚Äúdrone-hovering‚Äù. --ckpt 500 : Loads the trained policy from checkpoint 500. --record : Records the evaluation and saves a video of the drone‚Äôs performance. Visualize Results . The evaluation script will visualize the drone‚Äôs performance and save a video if the --record flag is set. By following this tutorial, you‚Äôll be able to train and evaluate a basic drone hovering policy using Genesis. Have fun and enjoy!
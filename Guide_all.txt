User Guide # Overview üí° What is Genesis üß¨ Why A New Physics Simulator üõ†Ô∏è Installation üéØ Genesis Vision & Mission Getting Started üëãüèª Hello, Genesis üì∏ Visualization & Rendering üïπÔ∏è Control Your Robot üöÄ Parallel Simulation ü¶æ Inverse Kinematics & Motion Planning üßó Advanced and Parallel IK üåä Beyond Rigid Bodies üßë‚Äçüíª Interactive Information Access and Debugging ü¶ø Training Locomotion Policies with RL üöÅ Training Drone Hovering Policies with RL üêõ Soft Robots üñ•Ô∏è Command Line Tools Advanced Topics üß© Concepts üí• Collision, Contacts & Forces ü™ê Differentiable Simulation üé± Collision Representations üí† Sparse Computation üßÆ Solvers & Coupling üõ∏ Drone
üßó Advanced and Parallel IK # The IK solver in Genesis has a lot of powerful features. In this example, we will show how you can configure your IK solver to accept more flexible target pose, and how you can solve for robots in a batched setting. IK with multiple end-effector links # In this example, we will use the left and right fingers of the robot gripper as two separate target links. In addition, instead of using a full 6-DoF pose as the target pose for each link, we only solve considering their positions and direction of the z-axis. import numpy as np import genesis as gs ########################## init ########################## gs . init ( seed = 0 , precision = '32' , logging_level = 'debug' ) ########################## create a scene ########################## scene = gs . Scene ( viewer_options = gs . options . ViewerOptions ( camera_pos = ( 2.0 , - 2 , 1.5 ), camera_lookat = ( 0.0 , 0.0 , 0.0 ), camera_fov = 40 , ), rigid_options = gs . options . RigidOptions ( enable_joint_limit = False , enable_collision = False , ), ) ########################## entities ########################## scene . add_entity ( gs . morphs . Plane (), ) robot = scene . add_entity ( gs . morphs . MJCF ( file = 'xml/franka_emika_panda/panda.xml' ), ) # two target links for visualization target_left = scene . add_entity ( gs . morphs . Mesh ( file = 'meshes/axis.obj' , scale = 0.1 , ), surface = gs . surfaces . Default ( color = ( 1 , 0.5 , 0.5 , 1 )), ) target_right = scene . add_entity ( gs . morphs . Mesh ( file = 'meshes/axis.obj' , scale = 0.1 , ), surface = gs . surfaces . Default ( color = ( 0.5 , 1.0 , 0.5 , 1 )), ) ########################## build ########################## scene . build () target_quat = np . array ([ 0 , 1 , 0 , 0 ]) center = np . array ([ 0.4 , - 0.2 , 0.25 ]) r = 0.1 left_finger = robot . get_link ( 'left_finger' ) right_finger = robot . get_link ( 'right_finger' ) for i in range ( 0 , 2000 ): target_pos_left = center + np . array ([ np . cos ( i / 360 * np . pi ), np . sin ( i / 360 * np . pi ), 0 ]) * r target_pos_right = target_pos_left + np . array ([ 0.0 , 0.03 , 0 ]) target_left . set_qpos ( np . concatenate ([ target_pos_left , target_quat ])) target_right . set_qpos ( np . concatenate ([ target_pos_right , target_quat ])) q = robot . inverse_kinematics_multilink ( links = [ left_finger , right_finger ], poss = [ target_pos_left , target_pos_right ], quats = [ target_quat , target_quat ], rot_mask = [ False , False , True ], # only restrict direction of z-axis ) # Note that this IK is for visualization purposes, so here we do not call scene.step(), but only update the state and the visualizer # In actual control applications, you should instead use robot.control_dofs_position() and scene.step() robot . set_dofs_position ( q ) scene . visualizer . update () This is what you will see: Here are a few new things we hope you could learn in this example: we used robot.inverse_kinematics_multilink() API for solving IK considering multiple target links. When using this API, we pass in a list of target link objects, a list of target positions, and a list of target orientations (quats). We used rot_mask to mask out directions of the axes we don‚Äôt care. In this example, we want both fingers to point downward, i.e. their Z-axis should point downward. However, we are less interested in restricting their rotation in the horizontal plane. You can use this rot_mask flexibly to achieve your desired goal pose. Similarly, there‚Äôs pos_mask you can use for masking out position along x/y/z axes. Since this example doesn‚Äôt involve any physics, after we set the position of the robot and the two target links, we don‚Äôt need to call physical simulation via scene.step() ; instead, we can only update the visualizer to reflect the change in the viewer (and camera, if any) by calling scene.visualizer.update() . What is qpos? Note that we used set_qpos for setting state of the target links. qpos represents an entity‚Äôs configuration in generalized coordinate. For a single arm, its qpos is identical to its dofs_position , and it has only 1 dof in all its joints (revolute + prismatic). For a free mesh that‚Äôs connected to world via a free joint, this joint has 6 dofs (3 translational + 3 rotational), while its generalized coordinate q is a 7-vector, which is essentially its xyz translation + wxyz quaternion, therefore, its qpos is different than its dofs_position . You can use both set_qpos() and set_dofs_position() to set its state, but since here we know the desired quaternion, it‚Äôs easier for us to compute the qpos . Shortly speaking, this difference comes from how we represent rotation, which can be represented as either a 3-vector (rotations around 3 axes) or a 4-vector (wxyz quaternion). IK for parallel simulation # Genesis allows you to solve IK even when you are in batched environments. Let‚Äôs spawn 16 parallel envs and let each of the robot‚Äôs end-effector rotation at a different angular speed: import numpy as np import genesis as gs ########################## init ########################## gs . init () ########################## create a scene ########################## scene = gs . Scene ( viewer_options = gs . options . ViewerOptions ( camera_pos = ( 0.0 , - 2 , 1.5 ), camera_lookat = ( 0.0 , 0.0 , 0.5 ), camera_fov = 40 , max_FPS = 200 , ), rigid_options = gs . options . RigidOptions ( enable_joint_limit = False , ), ) ########################## entities ########################## plane = scene . add_entity ( gs . morphs . Plane (), ) robot = scene . add_entity ( gs . morphs . MJCF ( file = 'xml/franka_emika_panda/panda.xml' ), ) ########################## build ########################## n_envs = 16 scene . build ( n_envs = n_envs , env_spacing = ( 1.0 , 1.0 )) target_quat = np . tile ( np . array ([ 0 , 1 , 0 , 0 ]), [ n_envs , 1 ]) # pointing downwards center = np . tile ( np . array ([ 0.4 , - 0.2 , 0.25 ]), [ n_envs , 1 ]) angular_speed = np . random . uniform ( - 10 , 10 , n_envs ) r = 0.1 ee_link = robot . get_link ( 'hand' ) for i in range ( 0 , 1000 ): target_pos = np . zeros ([ n_envs , 3 ]) target_pos [:, 0 ] = center [:, 0 ] + np . cos ( i / 360 * np . pi * angular_speed ) * r target_pos [:, 1 ] = center [:, 1 ] + np . sin ( i / 360 * np . pi * angular_speed ) * r target_pos [:, 2 ] = center [:, 2 ] target_q = np . hstack ([ target_pos , target_quat ]) q = robot . inverse_kinematics ( link = ee_link , pos = target_pos , quat = target_quat , rot_mask = [ False , False , True ], # for demo purpose: only restrict direction of z-axis ) robot . set_qpos ( q ) scene . step () When dealing with parallel envs, all you have to do is make sure you insert an extra batch dimension into your target pose variables.
üåä Beyond Rigid Bodies # Genesis unified multiple physics solvers and supports simulation beyond rigid body dynamics. A solver is essentially a set of physics simulation algorithms to handle a specific set of materials. In this tutorial, we will go through 3 popular solvers and use them to simulate entities with different physical properties: Smooth Particle Hydrodynamics (SPH) Solver Material Point Method (MPM) Solver Position Based Dynamics (PBD) Solver Liquid simulation using SPH Solver # First, let‚Äôs see how we can simulate a water cube. Let‚Äôs create an empty scene and add a plane as usual: import genesis as gs ########################## init ########################## gs . init () ########################## create a scene ########################## scene = gs . Scene ( sim_options = gs . options . SimOptions ( dt = 4e-3 , substeps = 10 , ), sph_options = gs . options . SPHOptions ( lower_bound = ( - 0.5 , - 0.5 , 0.0 ), upper_bound = ( 0.5 , 0.5 , 1 ), particle_size = 0.01 , ), vis_options = gs . options . VisOptions ( visualize_sph_boundary = True , ), show_viewer = True , ) ########################## entities ########################## plane = scene . add_entity ( morph = gs . morphs . Plane (), ) A few things to we should pay attention to here: When configuring sim_options , now we are using a relatively small dt with substeps=10 . This means inside the simulator, for each step , it will simulate 10 substep s, each with substep_dt = 4e-3 / 10 . When we were dealing with rigid bodies earlier, we didn‚Äôt need to set this and simply used the default setting ( substeps=1 ), which only runs 1 substeps in each step. Next, let‚Äôs add water. Adding As we discussed before, we use options to configure each different solver. Since we are using SPHSolver , we need to configure its properties via sph_options . In this example, we set the boundary of the solver, and specified the particle size to be 0.01m. SPHSolver is a lagrangian solver, meaning it uses particles to represent objects. In vis_options , we specified that we would like to see the boundary of the SPH Solver in the rendered view. Next, let‚Äôs add a water block entity and start the simulation! When we add the block, the only difference we need to make to turn it from a rigid block to a water block is setting the material . In fact, earlier when we were dealing with only rigid bodies, this was internally set to be gs.materials.Rigid() by default. Since we are now using the SPH Solver for liquid simulation, we select the Liquid material under the SPH category: liquid = scene . add_entity ( material = gs . materials . SPH . Liquid ( sampler = 'pbs' , ), morph = gs . morphs . Box ( pos = ( 0.0 , 0.0 , 0.65 ), size = ( 0.4 , 0.4 , 0.4 ), ), surface = gs . surfaces . Default ( color = ( 0.4 , 0.8 , 1.0 ), vis_mode = 'particle' , ), ) ########################## build ########################## scene . build () horizon = 1000 for i in range ( horizon ): scene . step () When creating the Liquid material, we set sampler='pbs' . This configures how we want to sample particles given the Box morph. pbs stands for ‚Äòphysics-based sampling‚Äô, which runs some extra simulation steps to make sure the particles are arranged in a physically natural way. You can also use 'regular' sampler to sample the particles simply using a grid lattice pattern. If you are using other solvers such as MPM, you can also use 'random' sampler. You may also note that we passed in an extra attribute ‚Äì surface . This attribute is used to define all the visual properties of the entity. Here, we set the color of the water to be blueish, and chose to visualize it as particles by setting vis_mod='particle' . Once you run this successfully, you will see the water drops and spreads over the plane, but constrained within the solver boundary: You can get the real-time particle positions by: particles = liquid . get_particles () Changing the liquid properties: You can also play with the physical properties of the liquid. For example, you can increase its viscosity ( mu ) and surface tension ( gamma ): material = gs . materials . SPH . Liquid ( mu = 0.02 , gamma = 0.02 ), and see how the behavior will be different. Enjoy! The complete script: import genesis as gs ########################## init ########################## gs . init () ########################## create a scene ########################## scene = gs . Scene ( sim_options = gs . options . SimOptions ( dt = 4e-3 , substeps = 10 , ), sph_options = gs . options . SPHOptions ( lower_bound = ( - 0.5 , - 0.5 , 0.0 ), upper_bound = ( 0.5 , 0.5 , 1 ), particle_size = 0.01 , ), vis_options = gs . options . VisOptions ( visualize_sph_boundary = True , ), show_viewer = True , ) ########################## entities ########################## plane = scene . add_entity ( morph = gs . morphs . Plane (), ) liquid = scene . add_entity ( # viscous liquid # material=gs.materials.SPH.Liquid(mu=0.02, gamma=0.02), material = gs . materials . SPH . Liquid (), morph = gs . morphs . Box ( pos = ( 0.0 , 0.0 , 0.65 ), size = ( 0.4 , 0.4 , 0.4 ), ), surface = gs . surfaces . Default ( color = ( 0.4 , 0.8 , 1.0 ), vis_mode = 'particle' , ), ) ########################## build ########################## scene . build () horizon = 1000 for i in range ( horizon ): scene . step () # get particle positions particles = liquid . get_particles () Deformable object simulation using MPM Solver # MPM solver is a very powerful physics solver that supports a wider range of materials. MPM stands for material point method, and uses a hybrid lagrangian-eulerian representation, i.e. both particles and grids, to represent objects. In this example, let‚Äôs create three objects: An elastic cube, visualized as 'particles' A liquid cube, visualized as 'particles' An elastoplastic sphere, visualized as the original sphere mesh, but deformed based on the internal particle state ( vis_mode='visual' ). Such process that maps internal particle state to a deformed visual mesh is called skinning in computer graphics. Complete code script: import genesis as gs ########################## init ########################## gs . init () ########################## create a scene ########################## scene = gs . Scene ( sim_options = gs . options . SimOptions ( dt = 4e-3 , substeps = 10 , ), mpm_options = gs . options . MPMOptions ( lower_bound = ( - 0.5 , - 1.0 , 0.0 ), upper_bound = ( 0.5 , 1.0 , 1 ), ), vis_options = gs . options . VisOptions ( visualize_mpm_boundary = True , ), viewer_options = gs . options . ViewerOptions ( camera_fov = 30 , ), show_viewer = True , ) ########################## entities ########################## plane = scene . add_entity ( morph = gs . morphs . Plane (), ) obj_elastic = scene . add_entity ( material = gs . materials . MPM . Elastic (), morph = gs . morphs . Box ( pos = ( 0.0 , - 0.5 , 0.25 ), size = ( 0.2 , 0.2 , 0.2 ), ), surface = gs . surfaces . Default ( color = ( 1.0 , 0.4 , 0.4 ), vis_mode = 'visual' , ), ) obj_sand = scene . add_entity ( material = gs . materials . MPM . Liquid (), morph = gs . morphs . Box ( pos = ( 0.0 , 0.0 , 0.25 ), size = ( 0.3 , 0.3 , 0.3 ), ), surface = gs . surfaces . Default ( color = ( 0.3 , 0.3 , 1.0 ), vis_mode = 'particle' , ), ) obj_plastic = scene . add_entity ( material = gs . materials . MPM . ElastoPlastic (), morph = gs . morphs . Sphere ( pos = ( 0.0 , 0.5 , 0.35 ), radius = 0.1 , ), surface = gs . surfaces . Default ( color = ( 0.4 , 1.0 , 0.4 ), vis_mode = 'particle' , ), ) ########################## build ########################## scene . build () horizon = 1000 for i in range ( horizon ): scene . step () Note that to change the underlying physical material, all you have to do is to change the material attribute. Feel free to play with other material types (such as MPM.Sand() and MPM.Snow() ), as well as the property values in each material type. Expected rendered result: Cloth simulation with PBD Solver # PBD stands for Position Based Dynamics. This is also a lagrangian solver that represents entities using particles and edges, and simulates their state by solving a set of position-based constraints. It can be used to simulate 1D/2D/3D entities that preserve their topologies. In this example, we will see how to simulate cloth with PBD solver. In this example, we will add two square-shape cloth entities: one with 4 corners fixed, the other with only 1 corner fixed and falls down onto the first piece of cloth. In addition, we will render them using different vis_mode s. Create the scene and build: import genesis as gs ########################## init ########################## gs . init () ########################## create a scene ########################## scene = gs . Scene ( sim_options = gs . options . SimOptions ( dt = 4e-3 , substeps = 10 , ), viewer_options = gs . options . ViewerOptions ( camera_fov = 30 , res = ( 1280 , 720 ), max_FPS = 60 , ), show_viewer = True , ) ########################## entities ########################## plane = scene . add_entity ( morph = gs . morphs . Plane (), ) cloth_1 = scene . add_entity ( material = gs . materials . PBD . Cloth (), morph = gs . morphs . Mesh ( file = 'meshes/cloth.obj' , scale = 2.0 , pos = ( 0 , 0 , 0.5 ), euler = ( 0.0 , 0 , 0.0 ), ), surface = gs . surfaces . Default ( color = ( 0.2 , 0.4 , 0.8 , 1.0 ), vis_mode = 'visual' , ) ) cloth_2 = scene . add_entity ( material = gs . materials . PBD . Cloth (), morph = gs . morphs . Mesh ( file = 'meshes/cloth.obj' , scale = 2.0 , pos = ( 0 , 0 , 1.0 ), euler = ( 0.0 , 0 , 0.0 ), ), surface = gs . surfaces . Default ( color = ( 0.8 , 0.4 , 0.2 , 1.0 ), vis_mode = 'particle' , ) ) ########################## build ########################## scene . build () Then, let‚Äôs fix the corners (particles) we want. To do this, we provide a handy tool to locate a particle using a location in the cartesian space: cloth_1 . fix_particle ( cloth_1 . find_closest_particle (( - 1 , - 1 , 1.0 ))) cloth_1 . fix_particle ( cloth_1 . find_closest_particle (( 1 , 1 , 1.0 ))) cloth_1 . fix_particle ( cloth_1 . find_closest_particle (( - 1 , 1 , 1.0 ))) cloth_1 . fix_particle ( cloth_1 . find_closest_particle (( 1 , - 1 , 1.0 ))) cloth_2 . fix_particle ( cloth_2 . find_closest_particle (( - 1 , - 1 , 1.0 ))) horizon = 1000 for i in range ( horizon ): scene . step () Expected rendered result: Warning Skinning for 2D meshes We noticed some issues when using a 2D flat cloth mesh and set vis_mode='visual' , this is due to degenerated pseudo-inverse matrix computation when computing the barycentric weights. You may notice weird visualization results in the above example if you add a non-zero euler to the cloth and use vis_mode='visual' . We will fix this very soon. More tutorials on inter-solver coupling coming soon!
üí• Collision, Contacts & Forces # Coming soon‚Ä¶
üé± Collision Representations # Coming soon‚Ä¶
üñ•Ô∏è Command Line Tools # We provided some command line tools that you can execute in terminal once Genesis is installed. These include: gs clean : Clean all the files cached by genesis and taichi gs view *.* : Visualize a given asset (mesh/URDF/MJCF) (can be useful if you want to quickly check if your asset can be loaded and visualized correctly) gs animate 'path/*.png' : Combine all images that matches the given pattern into a video.
üß© Concepts # Coming soon‚Ä¶
üïπÔ∏è Control Your Robot # Now that we have loaded an robot, let‚Äôs go through a comprehensive example to show you how you can control your robot via various ways. As usual, let‚Äôs import genesis, create a scene, and load a franka robot: import numpy as np import genesis as gs ########################## init ########################## gs . init ( backend = gs . gpu ) ########################## create a scene ########################## scene = gs . Scene ( viewer_options = gs . options . ViewerOptions ( camera_pos = ( 0 , - 3.5 , 2.5 ), camera_lookat = ( 0.0 , 0.0 , 0.5 ), camera_fov = 30 , max_FPS = 60 , ), sim_options = gs . options . SimOptions ( dt = 0.01 , ), show_viewer = True , ) ########################## entities ########################## plane = scene . add_entity ( gs . morphs . Plane (), ) # when loading an entity, you can specify its pose in the morph. franka = scene . add_entity ( gs . morphs . MJCF ( file = 'xml/franka_emika_panda/panda.xml' , pos = ( 1.0 , 1.0 , 0.0 ), euler = ( 0 , 0 , 0 ), ), ) ########################## build ########################## scene . build () This robot arm will fall down due to gravity, if we don‚Äôt give it any actuation force. Genesis has a built-in PD controller that takes as input target joint position or velocity. You can also directly set torque/force applied to each joint. In the context of robotic simulation, joint and dof (degree-of-freedom) are two related but different concepts. Since we are dealing with a Franka arm, which has 7 revolute joints in the arm and 2 prismatic joints in its gripper, all the joints have 1 dof only, leading to a 9-dof articulated body. In a more general case, there will be joint types such as free joint (6 dofs) or ball joint (3 dofs) that have more than one degrees of freedom. In general, you can think of each dof as a motor and can be controlled independently. In order to know which joint (dof) to control, we need to map the joint names we (as a user) defined in the URDF/MJCF file to the actual dof index inside the simulator: jnt_names = [ 'joint1' , 'joint2' , 'joint3' , 'joint4' , 'joint5' , 'joint6' , 'joint7' , 'finger_joint1' , 'finger_joint2' , ] dofs_idx = [ franka . get_joint ( name ) . dof_idx_local for name in jnt_names ] Note that here we are using .dof_idx_local to obtain the local idx of the dof with respect to the robot entity itself. You can also use joint.dof_idx to access each joint‚Äôs global dof index in the scene. Next, we can set the control gains for each dof. These gains determine how big the actual control force will be, given a target joint position or velocity. Usually, these information will be parsed from the imported MJCF or URDF file, but it‚Äôs always recommended to tune it manually or refer to a well-tuned value online. ############ Optional: set control gains ############ # set positional gains franka . set_dofs_kp ( kp = np . array ([ 4500 , 4500 , 3500 , 3500 , 2000 , 2000 , 2000 , 100 , 100 ]), dofs_idx_local = dofs_idx , ) # set velocity gains franka . set_dofs_kv ( kv = np . array ([ 450 , 450 , 350 , 350 , 200 , 200 , 200 , 10 , 10 ]), dofs_idx_local = dofs_idx , ) # set force range for safety franka . set_dofs_force_range ( lower = np . array ([ - 87 , - 87 , - 87 , - 87 , - 12 , - 12 , - 12 , - 100 , - 100 ]), upper = np . array ([ 87 , 87 , 87 , 87 , 12 , 12 , 12 , 100 , 100 ]), dofs_idx_local = dofs_idx , ) Note that these APIs in general takes as input two sets of values: the actual value to be set, and the corresponding dofs indices. Most control-related APIs follow this convention. Next, instead of using a physically-realistic PD controller, let‚Äôs first see how we can manually set the configuration of the robot. These APIs can make sudden changes to the robot state without obeying physics: # Hard reset for i in range ( 150 ): if i < 50 : franka . set_dofs_position ( np . array ([ 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0.04 , 0.04 ]), dofs_idx ) elif i < 100 : franka . set_dofs_position ( np . array ([ - 1 , 0.8 , 1 , - 2 , 1 , 0.5 , - 0.5 , 0.04 , 0.04 ]), dofs_idx ) else : franka . set_dofs_position ( np . array ([ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]), dofs_idx ) scene . step () If you have viewer turned on, you will see the robot changes state every 50 steps. Next, let‚Äôs try to control the robot using the built in PD controller. The API design in Genesis follows a structured pattern. We used set_dofs_position to hard set the dofs position. Now we simply changed set_* to control_* to use the controller counterpart APIs. Here we illustrate different ways for controlling the robot: # PD control for i in range ( 1250 ): if i == 0 : franka . control_dofs_position ( np . array ([ 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0.04 , 0.04 ]), dofs_idx , ) elif i == 250 : franka . control_dofs_position ( np . array ([ - 1 , 0.8 , 1 , - 2 , 1 , 0.5 , - 0.5 , 0.04 , 0.04 ]), dofs_idx , ) elif i == 500 : franka . control_dofs_position ( np . array ([ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]), dofs_idx , ) elif i == 750 : # control first dof with velocity, and the rest with position franka . control_dofs_position ( np . array ([ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ])[ 1 :], dofs_idx [ 1 :], ) franka . control_dofs_velocity ( np . array ([ 1.0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ])[: 1 ], dofs_idx [: 1 ], ) elif i == 1000 : franka . control_dofs_force ( np . array ([ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]), dofs_idx , ) # This is the control force computed based on the given control command # If using force control, it's the same as the given control command print ( 'control force:' , franka . get_dofs_control_force ( dofs_idx )) # This is the actual force experienced by the dof print ( 'internal force:' , franka . get_dofs_force ( dofs_idx )) scene . step () Let‚Äôs dive into it a bit: from step 0 to 500, we are using position control to control all the dofs, and move the robot to 3 target positions sequentially. Note that for control_* APIs, once a target value is set, it will be stored internally and you don‚Äôt need to send repetitive commands to the simulation in the following steps as long as your target stays the same. at step 750, we demonstrate we can do hybrid control for different dofs: for the first dof (dof 0), we send a velocity command, while the rest still follows position control commands at step 1000, we switch to torque (force) control and send a zero-force command to all the dofs, and the robot will again fall onto the floor due to gravity. At the end of each step, we print two types of forces: get_dofs_control_force() and get_dofs_force() . get_dofs_control_force() returns the force applied by the controller. In case of position or velocity control, this is computed using the target command and the control gains. In case of force (torque) control, this is same as the input control command get_dofs_force() returns the actual force experience by each dof, this is a combination of the force applied by the controller, and other internal forces such as collision force and coriolis force. If everything goes right, this is what you should see: Here is the full code script covering everything discussed above: import numpy as np import genesis as gs ########################## init ########################## gs . init ( backend = gs . gpu ) ########################## create a scene ########################## scene = gs . Scene ( viewer_options = gs . options . ViewerOptions ( camera_pos = ( 0 , - 3.5 , 2.5 ), camera_lookat = ( 0.0 , 0.0 , 0.5 ), camera_fov = 30 , res = ( 960 , 640 ), max_FPS = 60 , ), sim_options = gs . options . SimOptions ( dt = 0.01 , ), show_viewer = True , ) ########################## entities ########################## plane = scene . add_entity ( gs . morphs . Plane (), ) franka = scene . add_entity ( gs . morphs . MJCF ( file = 'xml/franka_emika_panda/panda.xml' , ), ) ########################## build ########################## scene . build () jnt_names = [ 'joint1' , 'joint2' , 'joint3' , 'joint4' , 'joint5' , 'joint6' , 'joint7' , 'finger_joint1' , 'finger_joint2' , ] dofs_idx = [ franka . get_joint ( name ) . dof_idx_local for name in jnt_names ] ############ Optional: set control gains ############ # set positional gains franka . set_dofs_kp ( kp = np . array ([ 4500 , 4500 , 3500 , 3500 , 2000 , 2000 , 2000 , 100 , 100 ]), dofs_idx_local = dofs_idx , ) # set velocity gains franka . set_dofs_kv ( kv = np . array ([ 450 , 450 , 350 , 350 , 200 , 200 , 200 , 10 , 10 ]), dofs_idx_local = dofs_idx , ) # set force range for safety franka . set_dofs_force_range ( lower = np . array ([ - 87 , - 87 , - 87 , - 87 , - 12 , - 12 , - 12 , - 100 , - 100 ]), upper = np . array ([ 87 , 87 , 87 , 87 , 12 , 12 , 12 , 100 , 100 ]), dofs_idx_local = dofs_idx , ) # Hard reset for i in range ( 150 ): if i < 50 : franka . set_dofs_position ( np . array ([ 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0.04 , 0.04 ]), dofs_idx ) elif i < 100 : franka . set_dofs_position ( np . array ([ - 1 , 0.8 , 1 , - 2 , 1 , 0.5 , - 0.5 , 0.04 , 0.04 ]), dofs_idx ) else : franka . set_dofs_position ( np . array ([ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]), dofs_idx ) scene . step () # PD control for i in range ( 1250 ): if i == 0 : franka . control_dofs_position ( np . array ([ 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0.04 , 0.04 ]), dofs_idx , ) elif i == 250 : franka . control_dofs_position ( np . array ([ - 1 , 0.8 , 1 , - 2 , 1 , 0.5 , - 0.5 , 0.04 , 0.04 ]), dofs_idx , ) elif i == 500 : franka . control_dofs_position ( np . array ([ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]), dofs_idx , ) elif i == 750 : # control first dof with velocity, and the rest with position franka . control_dofs_position ( np . array ([ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ])[ 1 :], dofs_idx [ 1 :], ) franka . control_dofs_velocity ( np . array ([ 1.0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ])[: 1 ], dofs_idx [: 1 ], ) elif i == 1000 : franka . control_dofs_force ( np . array ([ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ]), dofs_idx , ) # This is the control force computed based on the given control command # If using force control, it's the same as the given control command print ( 'control force:' , franka . get_dofs_control_force ( dofs_idx )) # This is the actual force experienced by the dof print ( 'internal force:' , franka . get_dofs_force ( dofs_idx )) scene . step ()
ü™ê Differentiable Simulation # (Under construction. More details coming soon) genesis.Tensor # We now have our own tensor data type: genesis.Tensor() , for the following reasons: to ensure a consistent user experience :) it enables end-to-end gradient flow from loss all the way back to action input it removes need for specifying datatype (though you still can) when creating tensors. The datatype specified when calling gs.init() will be used when creating genesis tensors. provides additional safety checks, such as contiguous check and check if tensors from different Scene are accidentally being merged into the same computation graph. supports other potential customizations if we need. This is essentially a subclass of pytorch tensors, so users can simply treat it as torch tensors and apply different kinds of torch operations. In pytorch, the recommended way of creating tensors is to call torch.tensor and other tensor creation ops like torch.rand , torch.zeros , torch.from_numpy , etc. We aim to reproduce the same experience in genesis, and genesis tensors can be created simply by replacing torch with genesis , e.g. x = gs . tensor ([ 0.5 , 0.73 , 0.5 ]) y = gs . rand ( size = ( horizon , 3 ), requires_grad = True ) Tensors created this way are leaf tensors, and their gradient can be accessed with tensor.grad after backward pass. Pytorch operations mixing torch and genesis tensors will automatically yield genesis tensors. There exist a few minor differences though: Similar to torch, genesis tensor creation automatically infers the data type of the tensor based on the parameter (whether int or float), but then will convert to the float or int type with precision specified in gs.init(). Users can also can override datatypes, and now we support dtype=int or dtype=float when calling the tensor creation ops. All genesis tensors are on cuda, so device selection is not allowed. This means unlike torch.from_numpy , genesis.from_numpy directly gives you tensors on cuda. genesis.from_torch(detach=True) : this creates a genesis tensor given a torch tensor. When calling this, if detach is True, the returned genesis tensor will be a new leaf node, detached from pytorch‚Äôs computation graph. If detach is False, the returned genesis tensor will be connected to the upstream torch computation graph, and when calling backward pass, the gradient will flow back all the way to connected pytorch tensors. By default, we should use purely genesis tensors, but this allows potential integration with upstream applications built on pytorch, e.g. training neural policies. Each genesis tensor object has a scene attribute. Any child tensors derived from it would inherit the same scene. This lets us keep track of the source of the gradient flow. state = scene . get_state () # state.pos is a genesis tensor print ( state . pos . scene ) # output: <class 'genesis.engine.scene.Scene'> id: 'e1a95be2-0947-4dcb-ad02-47b8541df0a0' random_tensor = gs . rand ( size = (), requires_grad = True ) print ( random_tensor . scene ) # output: None pos_ = state . pos + random_tensor print ( pos_ . scene ) # output: <class 'genesis.engine.scene.Scene'> id: 'e1a95be2-0947-4dcb-ad02-47b8541df0a0' If tensor.scene is None , tensor.backward() behaves identically to a torch tensor‚Äôs backward() . Otherwise, it will allow gradient flow back to tensor.scene and trigger upstream gradient flow. To resemble torch‚Äôs behavior like nn.Module.zero_grad() or optimizer.zero_grad() , you can also do tensor.zero_grad() with a genesis tensor.
üõ∏ Drone # Coming soon‚Ä¶
üëãüèª Hello, Genesis # In this tutorial, we will go through a basic example that loads a single Franka arm and then let it fall freely onto the floor, and use this example to illustrate the core steps for creating a simulation experiment in genesis, and some basic concepts: import genesis as gs gs . init ( backend = gs . cpu ) scene = gs . Scene ( show_viewer = True ) plane = scene . add_entity ( gs . morphs . Plane ()) franka = scene . add_entity ( gs . morphs . MJCF ( file = 'xml/franka_emika_panda/panda.xml' ), ) scene . build () for i in range ( 1000 ): scene . step () This is the complete code script ! Such an example only takes <10 lines of code, and already encapsulates all the necessary steps needed for creating a simulation experiment using genesis. You can stop here and start exploring genesis if you want, but if you are patient enough, let‚Äôs go through it step by step together: Initialization # The first step is to import genesis and initialize it: import genesis as gs gs . init ( backend = gs . cpu ) Backend device : Genesis is designed to be cross-platform, meaning that it supports various backend devices. Here we are using gs.cpu . If you need GPU-accelerated parallel simulation , you can switch to other backends such as gs.cuda , gs.vulkan or gs.metal . You can also use gs.gpu as a shortcut, and genesis will select a backend based on your system (e.g. gs.cuda if CUDA is available, and gs.metal for Apple Silicon devices). Precision level : By default, genesis uses f32 precision. You can change to f64 if you want a higher precision level by setting precision='64' . Logging level : Once genesis is initialized, you will see logger output on your terminal detailing your system info and genesis-related info like its current version. You can suppress logger output by setting logging_level to 'warning' . Color scheme : The default color theme used by genesis logger is optimized for dark background terminal, i.e. theme='dark' . You can change to 'light' if you are using a terminal with a light background, or simply use 'dumb' if you are a black-and-white person. A more detailed example of an gs.init() call would look like this: gs . init ( seed = None , precision = '32' , debug = False , eps = 1e-12 , logging_level = None , backend = gs_backend . gpu , theme = 'dark' , logger_verbose_time = False ) Create a scene # All the objects, robots, cameras, etc. in genesis are placed in a genesis Scene : scene = gs . Scene () A scene wraps a simulator object, which handles all the underlying physics solvers, and a visualizer object, which manages visualization-related concepts. For more details and APIs, see Scene . When creating a scene, there‚Äôs various physics solver parameters you can configure. A slightly more complex example would be: scene = gs . Scene ( sim_options = gs . options . SimOptions ( dt = 0.01 , gravity = ( 0 , 0 , - 10.0 ), ), show_viewer = True , viewer_options = gs . options . ViewerOptions ( camera_pos = ( 3.5 , 0.0 , 2.5 ), camera_lookat = ( 0.0 , 0.0 , 0.5 ), camera_fov = 40 , ), ) This example sets simulation dt to be 0.01s for each step, configures gravity, and sets the initial camera pose for the interactive viewer. Load objects into the scene # In this example, we load one plane and one franka arm into the scene: plane = scene . add_entity ( gs . morphs . Plane ()) franka = scene . add_entity ( gs . morphs . MJCF ( file = 'xml/franka_emika_panda/panda.xml' ), ) In genesis, all the objects and robots are represented as Entity . Genesis is designed to be fully object-oriented, so you will be able to interact with these entity objects through their methods directly, instead of using a handle or a global id assigned to them. The first parameter for add_entity is morph . A morph in Genesis is a hybrid concept, encapsulating both the geometry and pose information of an entity. By using different morphs, you can instantiate genesis entities from shape primitives, meshes, URDF, MJCF, Terrain, or soft robot description files. When creating the morph, you can additionally specify its position, orientation, size, etc. For orientation, a morph accepts either euler (scipy extrinsic x-y-z convention) or quat (w-x-y-z convention). One example would be: franka = scene . add_entity ( gs . morphs . MJCF ( file = 'xml/franka_emika_panda/panda.xml' , pos = ( 0 , 0 , 0 ), euler = ( 0 , 0 , 90 ), # we follow scipy's extrinsic x-y-z rotation convention, in degrees, # quat = (1.0, 0.0, 0.0, 0.0), # we use w-x-y-z convention for quaternions, scale = 1.0 , ), ) We currently support different types of shape primitives including: gs.morphs.Plane gs.morphs.Box gs.morphs.Cylinder gs.morphs.Sphere In addition, for training locomotion tasks, we support various types of built-in terrains as well as terrains initialized from user-given height maps via gs.morphs.Terrain , which we will cover in the following tutorials. We support loading from external files with different formats including : gs.morphs.MJCF : mujoco .xml robot configuration files gs.morphs.URDF : robot description files that end with .urdf (Unified Robotics Description Format) gs.morphs.Mesh : non-articulated mesh assets, supporting extensions including: *.obj , *.ply , *.stl , *.glb , *.gltf When loading from external files, you need to specify the file location using the file parameter. When parsing this, we support both absolute and relative file path. Note that since genesis also comes with an internal asset directory ( genesis/assets ), so if a relative path is used, we search not only relative path with respect to your current working directory, but also under genesis/assets . Therefore, in this example, we will retrieve the franka model from: genesis/assets/xml/franka_emika_panda/panda.xml . Note During genesis‚Äôs development, we have tried to support as many file extensions as we can, including support for loading their associated textures for rendering. If you would like us to support any other file types not listed above, or if you find your texture is not being loaded or rendered correctly, feel free to submit a feature request! If you want to load a Franka arm using an external URDF file, you can simply change the morph to gs.morphs.URDF(file='urdf/panda_bullet/panda.urdf', fixed=True) . Note that unlike MJCF file which already specifies the joint type connecting the robot‚Äôs base link and the world , URDF file doesn‚Äôt come with this information. Therefore, by default, the base link of a URDF robot tree is disconnected from the world (or more precisely, connected to world via a free 6-dof joint). Therefore, we need to additionally specify fixed=True for morphs.URDF and morphs.Mesh if we want the base link to be fixed. Build the scene and start simulating # scene . build () for i in range ( 1000 ): scene . step () Now that everything has been added, we can start the simulation. Note that we now need to build the scene first by calling scene.build() . This is because genesis uses just-in-time (JIT) technology to compile GPU kernels on the fly for each run, so we need an explicit step to initiate this process, which puts everything in place, allocates device memory, and creates underlying data fields for simulation. Once the scene is built, an interactive viewer will pop up to visualize the scene. The viewer comes with various keyboard shortcuts for video recording, screenshot, switching between different visualization modes, etc. We will discuss more details on visualization later in this tutorial. Note Kernel compilation and caching Due to the nature of JIT, each time you create a scene with a new configuration (i.e. different robot types, different number of objects, etc. that involves size change of the internal data structure), genesis needs to re-compile the GPU kernels on the fly. Genesis supports auto-caching of compiled kernels: after the first run (as long as it exits normally or is killed via ctrl + c , not ctrl + \ ), if the scene configuration stays the same, we will load from cached kernels from previous runs to speed up the scene creation process. We are actively working on optimizing this compilation step by adding techniques like parallel compilation and faster kernel serialization, so we expect to greatly speed up the speed of this step in future releases. Now we have walked through the whole example. Next, let‚Äôs dive into genesis‚Äôs visualization system, and play with the viewer and add some cameras.
üöÅ Training Drone Hovering Policies with RL # Genesis supports parallel simulation, making it ideal for training reinforcement learning (RL) drone hovering policies efficiently. In this tutorial, we will walk you through a complete training example for obtaining a basic hovering policy that enables a drone to maintain a stable hover position by reaching randomly generated target points. This is a simple and minimal example that demonstrates a very basic RL training pipeline in Genesis, and with the following example you will be able to obtain a drone hovering policy that‚Äôs deployable to a real drone very quickly. Note : This is NOT a comprehensive drone hovering policy training pipeline. It uses simplified reward terms to get you started easily, and does not exploit Genesis‚Äôs speed on big batch sizes, so it only serves basic demonstration purposes. Acknowledgement : This tutorial is inspired by Champion-level drone racing using deep reinforcement learning (Nature 2023) . Environment Overview # We start by creating a gym-style environment (hover-env). Initialize # The __init__ function sets up the simulation environment with the following steps: Control Frequency . The simulation runs at 100 Hz, providing a high-frequency control loop for the drone. Scene Creation . A simulation scene is created, including the drone and a static plane. Target Initialization . A random target point is initialized, which the drone will attempt to reach. Reward Registration . Reward functions, defined in the configuration, are registered to guide the policy. These functions will be explained in the ‚ÄúReward‚Äù section. Buffer Initialization . Buffers are initialized to store environment states, observations, and rewards. Reset # The reset_idx function resets the initial pose and state buffers of the specified environments. This ensures robots start from predefined configurations, crucial for consistent training. Step # The step function updates the environment state based on the actions taken by the policy. It includes the following steps: Action Execution . The input action will be clipped to a valid range, rescaled, and applied as adjustments to the default hover propeller RPMs. State Update . Drone states, such as position, attitude, and velocities, are retrieved and stored in buffers. Termination Checks . Terminated environments are reset automatically. Environments are terminated if Episode length exceeds the maximum allowed. The drone‚Äôs pitch or roll angle exceeds a specified threshold. The drone‚Äôs position exceeds specified boundaries. The drone is too close to the ground. Reward Computation . Rewards are calculated based on the drone‚Äôs performance in reaching the target point and maintaining stability. Observation Computation . Observations are normalized and returned to the policy. Observations used for training include drone‚Äôs position, attitude (quaternion), body linear velocity, body angular velocity and previous actions. Reward # In this example, we use the following reward functions to encourage the drone to reach the target point and maintain stability: target : Encourages the drone to reach the randomly generated target points. smooth : Encourages smooth actions and bridge the sim-to-real gap. yaw : Encourages the drone to maintain a stable hover yaw. angular : Encourages the drone to maintain low angular velocities. crash : Penalizes the drone for crashing or deviating too far from the target. These reward functions are combined to provide comprehensive feedback to the policy, guiding it to achieve stable and accurate hovering behavior. Training # At this stage, we have defined the environments. To train the drone hovering policy using PPO, follow these steps: Install Dependencies . Ensure you have installed all necessary dependencies, including Genesis and rsl_rl . # Install rsl_rl. git clone https://github.com/leggedrobotics/rsl_rl cd rsl_rl && git checkout v1.0.2 && pip install -e . # Install tensorboard. pip install tensorboard Run Training Script . Use the provided training script to start training the policy. python hover_train.py -e drone-hovering -B 8192 --max_iterations 500 -e drone-hovering : Specifies the experiment name as ‚Äúdrone-hovering‚Äù. -B 8192 : Sets the number of environments to 8192 for parallel training. --max_iterations 500 : Specifies the maximum number of training iterations to 500. To monitor the training process, launch TensorBoard: tensorboard --logdir logs You should see a training curve similar to this: Evaluation # To evaluate the trained drone hovering policy, follow these steps: Run Evaluation Script . Use the provided evaluation script to evaluate the trained policy. python hover_eval.py -e drone-hovering --ckpt 500 --record -e drone-hovering : Specifies the experiment name as ‚Äúdrone-hovering‚Äù. --ckpt 500 : Loads the trained policy from checkpoint 500. --record : Records the evaluation and saves a video of the drone‚Äôs performance. Visualize Results . The evaluation script will visualize the drone‚Äôs performance and save a video if the --record flag is set. By following this tutorial, you‚Äôll be able to train and evaluate a basic drone hovering policy using Genesis. Have fun and enjoy!
Roadmap # In progress and will be released in the near future # A differentiable & physics-based tactile sensor module Differentiable rigid body simulation Tiled rendering Faster JIT kernel compilation A comprehensive generative framework character motion camera motion interactive scene facial animation locomotion policy manipulation policy Boundless MPM simulation for large-scale environment Wanted feature, but not currently working on (Contributions needed!) # Viewer and headless rendering on windows An interactive GUI system Support for more MPM-based material models More sensor types
üõ†Ô∏è Installation # Prerequisites # Python : 3.9+ OS : Linux ( recommended ) / MacOS / Windows Note Genesis is designed to be cross-platform , supporting backend devices including CPU , CUDA GPU and non-CUDA GPU . That said, it is recommended to use Linux platform with CUDA-compatible GPU to achieve the best performance. Supported features on various systems are as follows: OS GPU Device GPU Simulation CPU Simulation Interactive Viewer Headless Rendering Linux Nvidia ‚úÖ ‚úÖ ‚úÖ ‚úÖ AMD ‚úÖ ‚úÖ ‚úÖ ‚úÖ Intel ‚úÖ ‚úÖ ‚úÖ ‚úÖ Windows Nvidia ‚úÖ ‚úÖ ‚ùå ‚ùå AMD ‚úÖ ‚úÖ ‚ùå ‚ùå Intel ‚úÖ ‚úÖ ‚ùå ‚ùå MacOS Apple Silicon ‚úÖ ‚úÖ ‚úÖ ‚úÖ Installation # Install PyTorch following the official instructions . Install Genesis via PyPI: pip install genesis-world Note If you are using Genesis with CUDA, make sure appropriate nvidia-driver is installed on your machine. (Optional) Motion planning # Genesis integrated OMPL‚Äôs motion planning functionalities and wraps it using a intuitive API for effortless motion planning. If you need the built-in motion planning capability, download pre-compiled OMPL wheel here , and then pip install it. (Optional) Surface reconstruction # If you need fancy visuals for visualizing particle-based entities (fluids, deformables, etc.), you typically need to reconstruct the mesh surface using the internal particle-based representation. We provide two options for this purpose: splashsurf , a state-of-the-art surface reconstruction method for achieving this: cargo install splashsurf ParticleMesher, our own openVDB-based surface reconstruction tool (faster but with not as smooth): echo "export LD_LIBRARY_PATH= ${ PWD } /ext/ParticleMesher/ParticleMesherPy: $LD_LIBRARY_PATH " >> ~/.bashrc source ~/.bashrc (Optional) Ray Tracing Renderer # If you need photo-realistic visuals, Genesis has a built-in a ray-tracing (path-tracing) based renderer developped using LuisaCompute , a high-performance domain specific language designed for rendering. 1. Get LuisaRender # The submodule LuisaRender is under ext/LuisaRender : git submodule update --init --recursive 2. Dependencies # 2.A: If you have sudo access. Preferred. # NB : It seems that compilation only works on Ubuntu 20.04+, As vulkan 1.2+ is needed and 18.04 only supports 1.1, but we haven‚Äôt fully checked this‚Ä¶ Upgrade g++ and gcc to version 11 sudo apt install build-essential manpages-dev software-properties-common sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt update && sudo apt install gcc-11 g++-11 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 110 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 110 # verify g++ --version gcc --version CMake # if your system's cmake version is under 3.18, uninstall that and reinstall via snap sudo snap install cmake --classic CUDA, a system-wide CUDA 12.0+ is needed. Download on https://developer.nvidia.com/cuda-12-1-0-download-archive Install CUDA Toolkit. Reboot. # verify nvcc --version Rust curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh sudo apt-get install patchelf # if the above gives downloader error, make sure your curl was installed via apt, not snap Vulkan sudo apt install libvulkan-dev zlib sudo apt-get install zlib1g-dev RandR headers sudo apt-get install xorg-dev libglu1-mesa-dev libsnappy sudo apt-get install libsnappy-dev pybind pip install "pybind11[global]" 2.B: If you have no sudo. # conda dependencies conda install -c conda-forge gcc = 11 .4 gxx = 11 .4 cmake = 3 .26.1 minizip zlib libuuid patchelf vulkan-tools vulkan-headers rust curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh pybind pip install "pybind11[global]" 3. Compile # Build LuisaRender and its python binding: If you used system dependencies (2.A) cd genesis/ext/LuisaRender cmake -S . -B build -D CMAKE_BUILD_TYPE = Release -D PYTHON_VERSIONS = 3 .9 -D LUISA_COMPUTE_DOWNLOAD_NVCOMP = ON -D LUISA_COMPUTE_ENABLE_GUI = OFF cmake --build build -j $( nproc ) By default, we use OptiX denoiser (For CUDA backend only). If you need OIDN denoiser, append -D LUISA_COMPUTE_DOWNLOAD_OIDN=ON . If you used conda dependencies (2.B) export CONDA_INCLUDE_PATH = path/to/anaconda/include cd ./ext/LuisaRender cmake -S . -B build -D CMAKE_BUILD_TYPE = Release -D PYTHON_VERSIONS = 3 .9 -D LUISA_COMPUTE_DOWNLOAD_NVCOMP = ON -D LUISA_COMPUTE_ENABLE_GUI = OFF -D ZLIB_INCLUDE_DIR = $CONDA_INCLUDE_PATH cmake --build build -j $( nproc ) The CONDA_INCLUDE_PATH typically looks like: /home/user/anaconda3/envs/genesis/include 4. FAQs # Assertion ‚Äòlerror‚Äô failed: Failed to write to the process: Broken pipe: You may need to use CUDA of the same version as compiled. if you followed 2.A and see ‚Äú GLIBCXX_3.4.30 not found‚Äù cd ~/anaconda3/envs/genesis/lib mv libstdc++.so.6 libstdc++.so.6.old ln -s /usr/lib/x86_64-linux-gnu/libstdc++.so.6 libstdc++.so.6
üßë‚Äçüíª Interactive Information Access and Debugging # We designed a very informative (and good-looking, hopefully) interface for accessing internal information and all the available attributes of objects created in Genesis, implemented via the __repr__() method for all the Genesis classes. This feature will be very useful if you are used to debugging via either IPython or pdb or ipdb . Let‚Äôs use IPython in this example. Install it via pip install ipython if you don‚Äôt have it. Let‚Äôs go through a simple example here: import genesis as gs gs . init () scene = gs . Scene ( show_viewer = False ) plane = scene . add_entity ( gs . morphs . Plane ()) franka = scene . add_entity ( gs . morphs . MJCF ( file = 'xml/franka_emika_panda/panda.xml' ), ) cam_0 = scene . add_camera () scene . build () # enter IPython's interactive mode import IPython ; IPython . embed () You can either run this script directly (if you have IPython installed), or you can just enter an IPython interactive window in terminal and past the code here without the last line. In this small block of code, we added a plane entity and a Franka arm. Now, if you are a newbie, you would probably be wondering what a scene actually contains. If you simply type scene in IPython (or ipdb or pdb or even a native python shell), you will see everything inside the scene, formatted and colorized nicely: In the top line, you will see the type of the object ( <gs.Scene> in this case). Then you will see all the available attributes inside it. For example, it tells you that the scene is built ( is_built is True ), its timestep ( dt ) is a float of value 0.01 seconds, and it unique id ( uid ) is '69be70e-dc9574f508c7a4c4de957ceb5' . The scene also has an attribute called solvers , which is essentially a list of different physics solvers it has. You can further type scene.solvers inside the shell and inspect this list, which is implemented using a gs.List class for better visualization: You can also inspect the Franka entity: Here you would see all the geoms and links it has and associated information. We can go one layer deeper, and type franka.links[0] : Here you will see all the collision geoms ( geoms ) and visual geoms ( vgeoms ) included in the link, and other important information such as its intertial_mass , the link‚Äôs global index in the scene ( idx ), which entity it belongs to ( entity , which is the franka arm entity), its joint ( joint ), etc. We hope this informative interface can make your debugging process easier!
ü¶æ Inverse Kinematics & Motion Planning # In this tutorial, we will go through several examples illustrating how to use solve inverse kinematics (IK) and motion planning in Genesis, and perform a simple grasping task. Let‚Äôs first create a scene, load your favorite robotic arm and a small cube, build the scene, and then set control gains: import numpy as np import genesis as gs ########################## init ########################## gs . init ( backend = gs . gpu ) ########################## create a scene ########################## scene = gs . Scene ( viewer_options = gs . options . ViewerOptions ( camera_pos = ( 3 , - 1 , 1.5 ), camera_lookat = ( 0.0 , 0.0 , 0.5 ), camera_fov = 30 , max_FPS = 60 , ), sim_options = gs . options . SimOptions ( dt = 0.01 , ), show_viewer = True , ) ########################## entities ########################## plane = scene . add_entity ( gs . morphs . Plane (), ) cube = scene . add_entity ( gs . morphs . Box ( size = ( 0.04 , 0.04 , 0.04 ), pos = ( 0.65 , 0.0 , 0.02 ), ) ) franka = scene . add_entity ( gs . morphs . MJCF ( file = 'xml/franka_emika_panda/panda.xml' ), ) ########################## build ########################## scene . build () motors_dof = np . arange ( 7 ) fingers_dof = np . arange ( 7 , 9 ) # set control gains # Note: the following values are tuned for achieving best behavior with Franka # Typically, each new robot would have a different set of parameters. # Sometimes high-quality URDF or XML file would also provide this and will be parsed. franka . set_dofs_kp ( np . array ([ 4500 , 4500 , 3500 , 3500 , 2000 , 2000 , 2000 , 100 , 100 ]), ) franka . set_dofs_kv ( np . array ([ 450 , 450 , 350 , 350 , 200 , 200 , 200 , 10 , 10 ]), ) franka . set_dofs_force_range ( np . array ([ - 87 , - 87 , - 87 , - 87 , - 12 , - 12 , - 12 , - 100 , - 100 ]), np . array ([ 87 , 87 , 87 , 87 , 12 , 12 , 12 , 100 , 100 ]), ) Next, let‚Äôs move the robot‚Äôs end-effector to a pre-grasping pose. This is done by two steps: using IK to solve the joint position given a target end-effector pose using a motion planner to reach the target position Motion planning in genesis uses OMPL library. You can install it following the instructions in the installation page. IK and motion planning in Genesis are as simple as it can get: each can be done via a single function call. # get the end-effector link end_effector = franka . get_link ( 'hand' ) # move to pre-grasp pose qpos = franka . inverse_kinematics ( link = end_effector , pos = np . array ([ 0.65 , 0.0 , 0.25 ]), quat = np . array ([ 0 , 1 , 0 , 0 ]), ) # gripper open pos qpos [ - 2 :] = 0.04 path = franka . plan_path ( qpos_goal = qpos , num_waypoints = 200 , # 2s duration ) # execute the planned path for waypoint in path : franka . control_dofs_position ( waypoint ) scene . step () # allow robot to reach the last waypoint for i in range ( 100 ): scene . step () As you can see, both IK solving and motion planning are two integrated methods of the robot entity. For IK solving, you simply tell the robot‚Äôs IK solver which link is the end-effector, and specify the target pose. Then, you tell the motion planner the target joint position (qpos) and it will return a planned and smoothed list of waypoints. Note that after we execute the path, we let the controller run for another 100 steps. This is because we are using a PD controller, and there will be a gap between the desired target position and the current position. Therefore, we let the controller run a bit longer so that the robot can reach the last waypoint in the planned trajectory. Next, we move the robot gripper down, grasp the cube, and lift it: # reach qpos = franka . inverse_kinematics ( link = end_effector , pos = np . array ([ 0.65 , 0.0 , 0.130 ]), quat = np . array ([ 0 , 1 , 0 , 0 ]), ) franka . control_dofs_position ( qpos [: - 2 ], motors_dof ) for i in range ( 100 ): scene . step () # grasp franka . control_dofs_position ( qpos [: - 2 ], motors_dof ) franka . control_dofs_force ( np . array ([ - 0.5 , - 0.5 ]), fingers_dof ) for i in range ( 100 ): scene . step () # lift qpos = franka . inverse_kinematics ( link = end_effector , pos = np . array ([ 0.65 , 0.0 , 0.28 ]), quat = np . array ([ 0 , 1 , 0 , 0 ]), ) franka . control_dofs_position ( qpos [: - 2 ], motors_dof ) for i in range ( 200 ): scene . step () When grasping the object, we used force control for the 2 gripper dofs, and applied a 0.5N grasping force. If everything goes right, you will see the object being grasped and lifted.
ü¶ø Training Locomotion Policies with RL # Genesis supports parallel simulation, making it ideal for training reinforcement learning (RL) locomotion policies efficiently. In this tutorial, we will walk you through a complete training example for obtaining a basic locomotion policy that enables a Unitree Go2 Robot to walk. This is a simple and minimal example that demonstrates a very basic RL training pipeline in Genesis, and with the following example you will be able to obtain a quadruped locomotion policy that‚Äôs deployable to a real robot very quickly. Note : This is NOT a comprehensive locomotion policy training pipeline. It uses simplified reward terms to get you started easily, and does not exploit Genesis‚Äôs speed on big batchsizes, so it only serves basic demonstration purposes. Acknowledgement : This tutorial is inspired by and builds several core concepts from Legged Gym . Environment Overview # We start by creating a gym-style environment (go2-env). Initialize # The __init__ function sets up the simulation environment with the following steps: Control Frequency . The simulation runs at 50 Hz, matching the real robot‚Äôs control frequency. To further bridge sim2real gap, we also manually simulate the action latecy (~20ms, one dt) shown on the real robot. Scene Creation . A simulation scene is created, including the robot and a static plane. PD Controller Setup . Motors are first identified based on their names. Stiffness and damping are then set for each motor. Reward Registration . Reward functions, defined in the configuration, are registered to guide the policy. These functions will be explained in the ‚ÄúReward‚Äù section. Buffer Initialization . Buffers are initialized to store environment states, observations, and rewards Reset # The reset_idx function resets the initial pose and state buffers of the specified environments. This ensures robots start from predefined configurations, crucial for consistent training. Step # The step function takes the action for execution and returns new observations and rewards. Here is how it works: Action Execution . The input action will be clipped, rescaled, and added on top of default motor positions. The transformed action, representing target joint positions, will then be sent to the robot controller for one-step execution. State Updates . Robot states, such as joint positions and velocities, are retrieved and stored in buffers. Termination Checks . Environments are terminated if (1) Episode length exceeds the maximum allowed (2) The robot‚Äôs body orientation deviates significantly. Terminated environments are reset automatically. Reward Computation . Observation Computation . Observation used for training includes base angular velocity, projected gravity, commands, dof position, dof velocity, and previous actions. Reward # Reward functions are critical for policy guidance. In this example, we use: tracking_lin_vel : Tracking of linear velocity commands (xy axes) tracking_ang_vel : Tracking of angular velocity commands (yaw) lin_vel_z : Penalize z axis base linear velocity action_rate : Penalize changes in actions base_height : Penalize base height away from target similar_to_default : Encourage the robot pose to be similar to the default pose Training # At this stage, we have defined the environments. Now, we use the PPO implementation from rsl-rl to train the policy. Follow these installation steps: # Install rsl_rl. git clone https : // github . com / leggedrobotics / rsl_rl cd rsl_rl && git checkout v1 .0.2 && pip install - e . # Install tensorboard. pip install tensorboard After installation, start training by running: python examples / locomotion / go2_train . py To monitor the training process, launch TensorBoard: tensorboard -- logdir logs You should see a training curve similar to this: Evaluation # Finally, let‚Äôs roll out the trained policy. Run the following command: python examples / locomotion / go2_eval . py You should see a GUI similar to this: If you happen to have a real Unitree Go2 robot by your side, you can try to deploy the policy. Have fun!
üéØ Genesis Vision & Mission # Simulation has played an crucial role in robotics research, providing a solid foundation for training robotic policies and generating data, leveraging the ever increasing power of computation. However, robotics researchers have long been limited by usability issues and opaqueness of available simulators. Existing GPU-accelerated simulators often come with steep learning curves due to intricate, data-centric concepts , complex APIs , and complicated software stacks ‚Äî making mastering them a daunting task for researchers, especially for newcomers in the field. Furthermore, some simulators are closed-source, restricting transparency and limiting researchers‚Äô ability to understand, control, or iteratively improve the underlying physics solvers based on e.g. real-world observations and feedback. The Genesis project was born from these challenges. Our vision is to build a fully transparent, user-friendly ecosystem where contributors from both physics simulation and robotics backgrounds can come together to collaboratively create a high-efficiency, realistic (both physically and visually) virtual world for robotics research and beyond . In addition, we recognize the wealth of innovative algorithms that are continuously developed by the computer graphics community for both simulation and rendering; however, there hasn‚Äôt been a collaborative effort so far that attempts to bring together these algorithms to create a realistic and compute-powered virtual realm for embodied and physical intelligence to emerge. So ‚Äì it‚Äôs about time! We aim to continuously develop Genesis and strive to make robotics research accessible and transparent to everyone. Meanwhile, we are aiming to bring the best of computer graphics technology into the robotics community, and build realistic virtual worlds for robots to explore and evolve. That‚Äôs what Genesis is all about ‚Äì new beginnings for robots, powered by an open and collaborative community. During our development, we have focused on making Genesis as easy to use as possible, with intuitive APIs and a clean, lightweight design. But we‚Äôre a small team of researchers, so our initial first release would by no means be perfect. We welcome contributors from any background to join us and improve Genesis in ways that could benefit the entire community. If you have feedback or suggestions while using Genesis ‚Äì whether it‚Äôs bug reports , feature requests , or even ideas to make the API design better ‚Äì please open an issue or submit a pull request on our GitHub repo . We‚Äôd love to hear your thoughts and we appreciate any forms of contributions!
üöÄ Parallel Simulation # The biggest advantage of using GPU to accelerate simulation is to enable scene-level parallelism, so that we can train robots in thousands of environments simultaneously. In Genesis, creating parallel simulation is as simple as you would imagine: when building your scene, you simply add an additional parameter n_envs to tell the simulator how many environments you want. That‚Äôs it. Note that in order to mimic the name convention in learning literature, we will also use the term batching to indicate the parallelization operation. Example script: import genesis as gs import torch ########################## init ########################## gs . init ( backend = gs . gpu ) ########################## create a scene ########################## scene = gs . Scene ( show_viewer = True , viewer_options = gs . options . ViewerOptions ( camera_pos = ( 3.5 , - 1.0 , 2.5 ), camera_lookat = ( 0.0 , 0.0 , 0.5 ), camera_fov = 40 , ), rigid_options = gs . options . RigidOptions ( dt = 0.01 , ), ) ########################## entities ########################## plane = scene . add_entity ( gs . morphs . Plane (), ) franka = scene . add_entity ( gs . morphs . MJCF ( file = 'xml/franka_emika_panda/panda.xml' ), ) ########################## build ########################## # create 20 parallel environments B = 20 scene . build ( n_envs = B , env_spacing = ( 1.0 , 1.0 )) # control all the robots franka . control_dofs_position ( torch . tile ( torch . tensor ([ 0 , 0 , 0 , - 1.0 , 0 , 0 , 0 , 0.02 , 0.02 ], device = gs . device ), ( B , 1 ) ), ) for i in range ( 1000 ): scene . step () The above script is almost identical to the example you see in Hello, Genesis , except scene.build() is now appended with two extra parameters: n_envs : this specifies how many batched environments you want to create env_spacing : the spawned parallel envs share identical states. For visualization purpose, you can specify this parameter to ask the visualizer to distribute all the envs in a grid with a distance of (x, y) in meters between each env. Note that this only affects the visualization behavior, and doesn‚Äôt change the actual position of the entities in each env. Control the robots in batched environments # Recall that we use APIs such as franka.control_dofs_position() in the previous tutorials. Now you can use the exact same API to control batched robots, except that the input variable needs an additional batch dimension: franka . control_dofs_position ( torch . zeros ( B , 9 , device = gs . device )) Since we are running simulation on GPU, in order to reduce data transfer overhead between cpu and gpu, we can use torch tensors selected using gs.device instead of numpy arrays (but numpy array will also work). This could bring noticeable performance gain when you need to send a tensor with a huge batch size frequently. The above call will control all the robots in the batched envs. If you want to control only a subset of environments, you can additionally pass in envs_idx , but make sure the size of the position tensor‚Äôs batch dimension matches the length of envs_idx : # control only 3 environments: 1, 5, and 7. franka . control_dofs_position ( position = torch . zeros ( 3 , 9 , device = gs . device ), envs_idx = torch . tensor ([ 1 , 5 , 7 ], device = gs . device ), ) This call will only send a zero-position command to 3 selected environments. Enjoy a futuristic speed! # Genesis supports up to tens of thousands of parallel environments, and unlocks unprecedented simulation speed this way. Now, let‚Äôs turn off the viewer, and change batch size to 30000 (consider using a smaller one if your GPU has a relatively small vram): import torch import genesis as gs gs . init ( backend = gs . gpu ) scene = gs . Scene ( show_viewer = False , rigid_options = gs . options . RigidOptions ( dt = 0.01 , ), ) plane = scene . add_entity ( gs . morphs . Plane (), ) franka = scene . add_entity ( gs . morphs . MJCF ( file = 'xml/franka_emika_panda/panda.xml' ), ) scene . build ( n_envs = 30000 ) # control all the robots franka . control_dofs_position ( torch . tile ( torch . tensor ([ 0 , 0 , 0 , - 1.0 , 0 , 0 , 0 , 0.02 , 0.02 ], device = gs . device ), ( 30000 , 1 ) ), ) for i in range ( 1000 ): scene . step () Running the above script on a desktop with RTX 4090 and 14900K gives you a futuristic simulation speed ‚Äì over 43 million frames per second, this is 430,000 faster than real-time. Enjoy! Tip FPS logging: By default, genesis logger will display real-time simulation speed in the terminal. You can disable this behavior by setting show_FPS=False when creating the scene.
üêõ Soft Robots # Volumetric muscle simulation # Genesis supports volumetric muscle simulation using MPM and FEM for soft robots. In the following example, we demonstrate an extremely simple soft robot with a sphere body, actuated by a sine-wave control signal. import numpy as np import genesis as gs ########################## init ########################## gs . init ( seed = 0 , precision = '32' , logging_level = 'debug' ) ########################## create a scene ########################## dt = 5e-4 scene = gs . Scene ( sim_options = gs . options . SimOptions ( substeps = 10 , gravity = ( 0 , 0 , 0 ), ), viewer_options = gs . options . ViewerOptions ( camera_pos = ( 1.5 , 0 , 0.8 ), camera_lookat = ( 0.0 , 0.0 , 0.0 ), camera_fov = 40 , ), mpm_options = gs . options . MPMOptions ( dt = dt , lower_bound = ( - 1.0 , - 1.0 , - 0.2 ), upper_bound = ( 1.0 , 1.0 , 1.0 ), ), fem_options = gs . options . FEMOptions ( dt = dt , damping = 45. , ), vis_options = gs . options . VisOptions ( show_world_frame = False , ), ) ########################## entities ########################## scene . add_entity ( morph = gs . morphs . Plane ()) E , nu = 3.e4 , 0.45 rho = 1000. robot_mpm = scene . add_entity ( morph = gs . morphs . Sphere ( pos = ( 0.5 , 0.2 , 0.3 ), radius = 0.1 , ), material = gs . materials . MPM . Muscle ( E = E , nu = nu , rho = rho , model = 'neohooken' , ), ) robot_fem = scene . add_entity ( morph = gs . morphs . Sphere ( pos = ( 0.5 , - 0.2 , 0.3 ), radius = 0.1 , ), material = gs . materials . FEM . Muscle ( E = E , nu = nu , rho = rho , model = 'stable_neohooken' , ), ) ########################## build ########################## scene . build () ########################## run ########################## scene . reset () for i in range ( 1000 ): actu = np . array ([ 0.2 * ( 0.5 + np . sin ( 0.01 * np . pi * i ))]) robot_mpm . set_actuation ( actu ) robot_fem . set_actuation ( actu ) scene . step () This is what you will see: Most of the code is pretty standard compared to instantiating regular deformable entities. There are only two small differences that do the trick: When instantiating soft robots robot_mpm and robot_fem , we use materials gs.materials.MPM.Muscle and gs.materials.FEM.Muscle respectively. When stepping the simulation, we use robot_mpm.set_actuation or robot_fem.set_actuation to set the actuation of the muscle. By default, there is only one muscle that spans the entire robot body with the muscle direction perpendicular to the ground [0, 0, 1] . In the next example, we show how to simulate a worm crawling forward by setting muscle groups and directions, as shown in the following. (The full script can be found in tutorials/advanced_worm.py .) ########################## entities ########################## worm = scene . add_entity ( morph = gs . morphs . Mesh ( file = 'meshes/worm/worm.obj' , pos = ( 0.3 , 0.3 , 0.001 ), scale = 0.1 , euler = ( 90 , 0 , 0 ), ), material = gs . materials . MPM . Muscle ( E = 5e5 , nu = 0.45 , rho = 10000. , model = 'neohooken' , n_groups = 4 , ), ) ########################## set muscle ########################## def set_muscle_by_pos ( robot ): if isinstance ( robot . material , gs . materials . MPM . Muscle ): pos = robot . get_state () . pos n_units = robot . n_particles elif isinstance ( robot . material , gs . materials . FEM . Muscle ): pos = robot . get_state () . pos [ robot . get_el2v ()] . mean ( 1 ) n_units = robot . n_elements else : raise NotImplementedError pos = pos . cpu () . numpy () pos_max , pos_min = pos . max ( 0 ), pos . min ( 0 ) pos_range = pos_max - pos_min lu_thresh , fh_thresh = 0.3 , 0.6 muscle_group = np . zeros (( n_units ,), dtype = int ) mask_upper = pos [:, 2 ] > ( pos_min [ 2 ] + pos_range [ 2 ] * lu_thresh ) mask_fore = pos [:, 1 ] < ( pos_min [ 1 ] + pos_range [ 1 ] * fh_thresh ) muscle_group [ mask_upper & mask_fore ] = 0 # upper fore body muscle_group [ mask_upper & ~ mask_fore ] = 1 # upper hind body muscle_group [ ~ mask_upper & mask_fore ] = 2 # lower fore body muscle_group [ ~ mask_upper & ~ mask_fore ] = 3 # lower hind body muscle_direction = np . array ([[ 0 , 1 , 0 ]] * n_units , dtype = float ) robot . set_muscle ( muscle_group = muscle_group , muscle_direction = muscle_direction , ) set_muscle_by_pos ( worm ) ########################## run ########################## scene . reset () for i in range ( 1000 ): actu = np . array ([ 0 , 0 , 0 , 1. * ( 0.5 + np . sin ( 0.005 * np . pi * i ))]) worm . set_actuation ( actu ) scene . step () This is what you will see: Several things that worth noticing in this code snippet: When specifying the material gs.materials.MPM.Muscle , we set an additional argument n_groups = 4 , which means there can be at most 4 different muscles in this robot. We can set the muscle by calling robot.set_muscle , which takes muscle_group and muscle_direction as inputs. Both have the same length as n_units , where in MPM n_units is the number of particles while in FEM n_units is the number of elements. muscle_group is an array of integer ranging from 0 to n_groups - 1 , indicating which muscle group an unit of the robot body belongs to. muscle_direction is an array of floating-point numbers that specify vectors for muscle direction. Note that we don‚Äôt do normalization and thus you may want to make sure the input muscle_direction is already normalized. How we set the muscle of this worm example is simply breaking the body into four parts: upper fore, upper hind, lower fore, and lower hind body, using lu_thresh for thresholding between lower/upper and fh_thresh for thresholding between fore/hind. Now given four muscle groups, when setting the control via set_actuation , the actuation input is thus an array of shape (4,) . Hybrid (rigid-and-soft) robot # Another type of soft robot is using rigid-bodied inner skeleton to actuatuate soft-bodied outer skin, or more precisely speaking, hybrid robot. With both rigid-bodied and soft-bodied dynamics implemented already, Genesis also supports hybrid robot. The following example is a hybrid robot with a two-link skeleton wrapped by soft skin pushing a rigid ball. import numpy as np import genesis as gs ########################## init ########################## gs . init ( seed = 0 , precision = '32' , logging_level = 'debug' ) ######################## create a scene ########################## dt = 3e-3 scene = gs . Scene ( sim_options = gs . options . SimOptions ( substeps = 10 , ), viewer_options = gs . options . ViewerOptions ( camera_pos = ( 1.5 , 1.3 , 0.5 ), camera_lookat = ( 0.0 , 0.0 , 0.0 ), camera_fov = 40 , ), rigid_options = gs . options . RigidOptions ( dt = dt , gravity = ( 0 , 0 , - 9.8 ), enable_collision = True , enable_self_collision = False , ), mpm_options = gs . options . MPMOptions ( dt = dt , lower_bound = ( 0.0 , 0.0 , - 0.2 ), upper_bound = ( 1.0 , 1.0 , 1.0 ), gravity = ( 0 , 0 , 0 ), # mimic gravity compensation enable_CPIC = True , ), vis_options = gs . options . VisOptions ( show_world_frame = True , visualize_mpm_boundary = False , ), ) ########################## entities ########################## scene . add_entity ( morph = gs . morphs . Plane ()) robot = scene . add_entity ( morph = gs . morphs . URDF ( file = "urdf/simple/two_link_arm.urdf" , pos = ( 0.5 , 0.5 , 0.3 ), euler = ( 0.0 , 0.0 , 0.0 ), scale = 0.2 , fixed = True , ), material = gs . materials . Hybrid ( mat_rigid = gs . materials . Rigid ( gravity_compensation = 1. , ), mat_soft = gs . materials . MPM . Muscle ( # to allow setting group E = 1e4 , nu = 0.45 , rho = 1000. , model = 'neohooken' , ), thickness = 0.05 , damping = 1000. , func_instantiate_rigid_from_soft = None , func_instantiate_soft_from_rigid = None , func_instantiate_rigid_soft_association = None , ), ) ball = scene . add_entity ( morph = gs . morphs . Sphere ( pos = ( 0.8 , 0.6 , 0.1 ), radius = 0.1 , ), material = gs . materials . Rigid ( rho = 1000 , friction = 0.5 ), ) ########################## build ########################## scene . build () ########################## run ########################## scene . reset () for i in range ( 1000 ): dofs_ctrl = np . array ([ 1. * np . sin ( 2 * np . pi * i * 0.001 ), ] * robot . n_dofs ) robot . control_dofs_velocity ( dofs_ctrl ) scene . step () This is what you will see: You can specify hybrid robot with the material gs.materials.Hybrid , which consists of gs.materials.Rigid and gs.materials.MPM.Muscle . Note that only MPM is supported here and it must be the Muscle class since the hybrid material reuses internally the muscle_group implemented for Muscle . When controlling the robot, given the actuation being from the inner rigid-bodied skeleton, there is a similar interface to rigid-bodied robot, e.g., control_dofs_velocity , control_dofs_force , control_dofs_position . Also, the control dimension is the same as the DoFs of the inner skeleton (in the above example, 2). The skin is determined by the shape of the inner skeleton, where thickness determines the skin thickness when wrapping the skeleton. By default, we grow skin based on the shape of the skeleton, which is specified by morph (in this example, the urdf/simple/two_link_arm.urdf ). The argument func_instantiate_soft_from_rigid of gs.materials.Hybrid defines concretely how skin should grow based on the rigid-bodied morph . There is a default implementation default_func_instantiate_soft_from_rigid in genesis/engine/entities/hybrid_entity.py . You can also implement your own function. When morph is Mesh instead of URDF , the mesh specifies the soft outer body and the inner skeleton is grown based on the skin shape. This is defined by func_instantiate_rigid_from_soft . There is also a default implementation default_func_instantiate_rigid_from_soft , which basically implements skeletonization of 3D meshes. The argument func_instantiate_rigid_soft_association of gs.materials.Hybrid determines how each skeletal part is associated with skin. The default implementation is to find the closest particles of the soft skin to the rigid skeletal parts.
üßÆ Solvers & Coupling # Coming soon‚Ä¶
üí† Sparse Computation # Coming soon‚Ä¶
üì∏ Visualization & Rendering # Genesis‚Äôs visualization system is managed by the visualizer of the scene you just created (i.e. scene.visualizer ). There are two ways for visualizing the scene: 1). using the interactive viewer that runs in a separate thread, and 2). by manually adding cameras to the scene and render images using the camera. Viewer # If you are connected to a display, you can visualize the scene using the interactive viewer. Genesis uses different options groups to configure different components in the scene. To configure the viewer, you can change the parameters in viewer_options when creating the scene. In addition, we use vis_options to specify visualization-related properties, which will be shared by the viewer and cameras (that we will add very soon). Create a scene with a more detailed viewer and vis setting (this looks a bit complex, but it‚Äôs just for illustration purposes): scene = gs . Scene ( show_viewer = True , viewer_options = gs . options . ViewerOptions ( res = ( 1280 , 960 ), camera_pos = ( 3.5 , 0.0 , 2.5 ), camera_lookat = ( 0.0 , 0.0 , 0.5 ), camera_fov = 40 , max_FPS = 60 , ), vis_options = gs . options . VisOptions ( show_world_frame = True , # visualize the coordinate frame of `world` at its origin world_frame_size = 1.0 , # length of the world frame in meter show_link_frame = False , # do not visualize coordinate frames of entity links show_cameras = False , # do not visualize mesh and frustum of the cameras added plane_reflection = True , # turn on plane reflection ambient_light = ( 0.1 , 0.1 , 0.1 ), # ambient light setting ), renderer = gs . renderers . Rasterizer (), # using rasterizer for camera rendering ) Here we can specify the pose and fov of the viewer camera. The viewer will run as fast as possible if max_FPS is set to None . If res is set to None, genesis will automatically create a 4:3 window with the height set to half of your display height. Also note that in the above setting, we set to use rasterization backend for camera rendering. Genesis provides two rendering backends: gs.renderers.Rasterizer() and gs.renderers.RayTracer() . The viewer always uses the rasterizer. By default, camera also uses rasterizer. Once the scene is created, you can access the viewer object via scene.visualizer.viewer , or simply scene.viewer as a shortcut. You can query or set the viewer camera pose: cam_pose = scene . viewer . camera_pose scene . viewer . set_camera_pose ( cam_pose ) Camera & Headless Rendering # Now let‚Äôs manually add a camera object to the scene. Cameras are not connected to the viewer or the display, and returns rendered images only when you need it. Therefore, camera works in headless mode. cam = scene . add_camera ( res = ( 1280 , 960 ), pos = ( 3.5 , 0.0 , 2.5 ), lookat = ( 0 , 0 , 0.5 ), fov = 30 , GUI = False ) If GUI=True , each camera will create an opencv window to dynamically display the rendered image. Note that this is different from the viewer GUI. Then, once we build the scene, we can render images using the camera. Our camera supports rendering rgb image, depth, segmentation mask and surface normals. By default, only rgb is rendered, and you can turn other modes on by setting the parameters when calling camera.render() : scene . build () # render rgb, depth, segmentation mask and normal map rgb , depth , segmentation , normal = cam . render ( depth = True , segmentation = True , normal = True ) If you used GUI=True and have a display connected, you should be able to see 4 windows now. (Sometimes opencv windows comes with extra delay, so you can call extra cv2.waitKey(1) if the windows are black, or simply call render() again to refresh the window.) Record videos using camera Now, let‚Äôs only render rgb images, and move the camera around and record a video. Genesis provides a handy util for recording videos: # start camera recording. Once this is started, all the rgb images rendered will be recorded internally cam . start_recording () import numpy as np for i in range ( 120 ): scene . step () # change camera position cam . set_pose ( pos = ( 3.0 * np . sin ( i / 60 ), 3.0 * np . cos ( i / 60 ), 2.5 ), lookat = ( 0 , 0 , 0.5 ), ) cam . render () # stop recording and save video. If `filename` is not specified, a name will be auto-generated using the caller file name. cam . stop_recording ( save_to_filename = 'video.mp4' , fps = 60 ) You will have the video saved to video.mp4 : Here is the full code script covering everything discussed above: import genesis as gs gs . init ( backend = gs . cpu ) scene = gs . Scene ( show_viewer = True , viewer_options = gs . options . ViewerOptions ( res = ( 1280 , 960 ), camera_pos = ( 3.5 , 0.0 , 2.5 ), camera_lookat = ( 0.0 , 0.0 , 0.5 ), camera_fov = 40 , max_FPS = 60 , ), vis_options = gs . options . VisOptions ( show_world_frame = True , world_frame_size = 1.0 , show_link_frame = False , show_cameras = False , plane_reflection = True , ambient_light = ( 0.1 , 0.1 , 0.1 ), ), renderer = gs . renderers . Rasterizer (), ) plane = scene . add_entity ( gs . morphs . Plane (), ) franka = scene . add_entity ( gs . morphs . MJCF ( file = 'xml/franka_emika_panda/panda.xml' ), ) cam = scene . add_camera ( res = ( 640 , 480 ), pos = ( 3.5 , 0.0 , 2.5 ), lookat = ( 0 , 0 , 0.5 ), fov = 30 , GUI = False , ) scene . build () # render rgb, depth, segmentation, and normal # rgb, depth, segmentation, normal = cam.render(rgb=True, depth=True, segmentation=True, normal=True) cam . start_recording () import numpy as np for i in range ( 120 ): scene . step () cam . set_pose ( pos = ( 3.0 * np . sin ( i / 60 ), 3.0 * np . cos ( i / 60 ), 2.5 ), lookat = ( 0 , 0 , 0.5 ), ) cam . render () cam . stop_recording ( save_to_filename = 'video.mp4' , fps = 60 ) Photo-realistic Ray Tracing Rendering # Genesis provides a ray tracing rendering backend for photorealistic rendering. You can easily switch to using this backend by setting renderer=gs.renderers.RayTracer() when creating the scene. This camera allows more parameter adjustment, such as spp , aperture , model , etc. Setup # Tested on Ubuntu 22.04, CUDA 12.4, python 3.9 Get submodules, specifically genesis/ext/LuisaRender . # inside Genesis/ git submodule update --init --recursive pip install -e ".[render]" Install/upgrad g++ and gcc (to) version 11. sudo apt install build-essential manpages-dev software-properties-common sudo add-apt-repository ppa:ubuntu-toolchain-r/test sudo apt update && sudo apt install gcc-11 g++-11 sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 110 sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 110 # verify version g++ --version gcc --version Install cmake. We use snap instead of apt because we need its version >= 3.26. However, remember to use the correct cmake. You may have /usr/local/bin/cmake but the snap installed package is at /snap/bin/cmake (or /usr/bin/snap ). Please double check the order of binary path via echo $PATH . sudo snap install cmake --classic cmake --version Install dependencies, sudo apt install libvulkan-dev # Vulkan sudo apt-get install zlib1g-dev # zlib sudo apt-get install libx11-dev # X11 sudo apt-get install xorg-dev libglu1-mesa-dev # RandR headers Build LuisaRender . Remember to use the correct cmake. cd genesis/ext/LuisaRender cmake -S . -B build -D CMAKE_BUILD_TYPE = Release -D PYTHON_VERSIONS = 3 .9 -D LUISA_COMPUTE_DOWNLOAD_NVCOMP = ON # remember to check python version cmake --build build -j $( nproc ) If you really struggle getting the build, we have some build here and you can check if your machine happens to have the same setup. The naming follows build_<commit-tag>_cuda<version>_python<version> . Download the one that matches your system, rename to build/ and put it in genesis/ext/LuisaRender . Finally, you can run the example, cd examples/rendering python demo.py You should be able to get FAQ # Pybind error when doing cmake -S . -B build -D CMAKE_BUILD_TYPE=Release -D PYTHON_VERSIONS=3.9 -D LUISA_COMPUTE_DOWNLOAD_NVCOMP=ON , CMake Error at src/apps/CMakeLists.txt:12 ( find_package ) : By not providing "Findpybind11.cmake" in CMAKE_MODULE_PATH this project has asked CMake to find a package configuration file provided by "pybind11" , but CMake did not find one. Could not find a package configuration file provided by "pybind11" with any of the following names: pybind11Config.cmake pybind11-config.cmake You probably forget to do pip install -e ".[render]" . Alternatively, you can simply do pip install "pybind11[global]" . CUDA runtime compilation error when doing cmake -S . -B build -D CMAKE_BUILD_TYPE=Release -D PYTHON_VERSIONS=3.9 -D LUISA_COMPUTE_DOWNLOAD_NVCOMP=ON , /usr/bin/ld: CMakeFiles/luisa-cuda-nvrtc-standalone-compiler.dir/cuda_nvrtc_compiler.cpp.o: in function `main': cuda_nvrtc_compiler.cpp:(.text.startup+0x173): undefined reference to `nvrtcGetOptiXIRSize' /usr/bin/ld: cuda_nvrtc_compiler.cpp:(.text.startup+0x197): undefined reference to `nvrtcGetOptiXIR' You need to install ‚Äúsystem-wise‚Äù cuda-toolkit ( official installation guide ). You first check the cuda-toolkit, nvcc --version # this should be the consistent with you cuda version from nvidia-smi which nvcc # just to check you are using the cuda-toolkit you expected If you don‚Äôt get proper output from nvcc , please follow the official cuda-toolkit installation guide. Yet, just as an example of installing cuda-toolkit for cuda-12.4. Download installer as in here . wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda-repo-ubuntu2204-12-4-local_12.4.0-550.54.14-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu2204-12-4-local_12.4.0-550.54.14-1_amd64.deb sudo cp /var/cuda-repo-ubuntu2204-12-4-local/cuda-*-keyring.gpg /usr/share/keyrings/ sudo apt-get update sudo apt-get -y install cuda-toolkit-12-4 Remember to set binary and runtime library path. In ~/.bashrc , add the following (note that we append the CUDA path at the end since there are also another gcc and g++ in /usr/local/cuda-12.4/bin and may not be version 11, which is required for the build), PATH = ${ PATH :+ ${ PATH } : } /usr/local/cuda-12.4/bin LD_LIBRARY_PATH = ${ LD_LIBRARY_PATH :+: ${ LD_LIBRARY_PATH }} /usr/local/cuda-12.4/lib64 Remember to either restart the terminal or do source ~/.bashrc . Another type of error is, <your-env-path>/bin/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_fatal_printf@GLIBC_PRIVATE' <your-env-path>/bin/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_audit_symbind_alt@GLIBC_PRIVATE' <your-env-path>/genesis-test1/bin/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_exception_create@GLIBC_PRIVATE' <your-env-path>/bin/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `__nptl_change_stack_perm@GLIBC_PRIVATE' <your-env-path>/bin/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `__tunable_get_val@GLIBC_PRIVATE' <your-env-path>/bin/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_audit_preinit@GLIBC_PRIVATE' <your-env-path>/bin/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_find_dso_for_object@GLIBC_PRIVATE' This may be due to the cuda-toolkit in your conda environment. Please do the following and install the system-wise CUDA, which nvcc conda uninstall cuda-toolkit Alternatively, you can add your conda library path to the runtime library path, ls $CONDA_PREFIX /lib/libcudart.so # you should have this # inside you ~/.bashrc, add LD_LIBRARY_PATH = ${ LD_LIBRARY_PATH :+: ${ LD_LIBRARY_PATH }} /usr/local/cuda-12.4/lib64 Lastly, remember to clear the build after doing the above fixed, rm -r build Compiler error at cmake -S . -B build -D CMAKE_BUILD_TYPE=Release -D PYTHON_VERSIONS=3.9 -D LUISA_COMPUTE_DOWNLOAD_NVCOMP=ON , CMake Error at /snap/cmake/1435/share/cmake-3.31/Modules/CMakeDetermineCCompiler.cmake:49 ( message ) : Could not find compiler set in environment variable CC: /home/tsunw/miniconda3/envs/genesis-test1/bin/x86_64-conda-linux-gnu-cc. Call Stack ( most recent call first ) : CMakeLists.txt:21 ( project ) CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage You are probably not using gcc and g++ version 11. Please double check (i) the version (ii) if the binary points to the path as expected (iii) the order of your binary path, gcc --version g++ --version which gcc which g++ echo $PATH # e.g., /usr/local/cuda-12.4/bin/gcc (version = 10.5) shouldn't be in front of /usr/bin/gcc (version = 11 if you install properly with apt) Import error when running examples/rendering/demo.py , [Genesis] [11:29:47] [ERROR] Failed to import LuisaRenderer. ImportError: /home/tsunw/miniconda3/envs/genesis-test1/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /home/tsunw/workspace/Genesis/genesis/ext/LuisaRender/build/bin/liblc-core.so) Conda‚Äôs libstdc++.so.6 doesn‚Äôt support 3.4.30. You need to move system‚Äôs into conda ( reference ). cd $CONDA_PREFIX /lib mv libstdc++.so.6 libstdc++.so.6.old ln -s /usr/lib/x86_64-linux-gnu/libstdc++.so.6 libstdc++.so.6
üí° What is Genesis # Genesis is a physics platform designed for general purpose Robotics/Embodied AI/Physical AI applications. It is simultaneously multiple things: A universal physics engine re-built from the ground up, capable of simulating a wide range of materials and physical phenomena. A lightweight , ultra-fast , pythonic , and user-friendly robotics simulation platform. A powerful and fast photo-realistic rendering system . A generative data engine that transforms user-prompted natural language description into various modalities of data. Powered by a universal physics engine re-designed and re-built from the ground up, Genesis integrates various physics solvers and their coupling into a unified framework. This core physics engine is further enhanced by a generative agent framework that operates at an upper level, aiming towards fully automated data generation for robotics and beyond. Currently, we are open-sourcing the underlying physics engine and the simulation platform. The generative framework will be released in the near future. Genesis is built and will continuously evolve with the following long-term missions : Lowering the barrier to using physics simulations and making robotics research accessible to everyone. (See our commitment ) Unifying a wide spectrum of state-of-the-art physics solvers into a single framework, allowing re-creating the whole physical world in a virtual realm with the highest possible physical, visual and sensory fidelity, using the most advanced simulation techniques. Minimizing human effort in collecting and generating data for robotics and other domains, letting the data flywheel spin on its own.
üß¨ Why A New Physics Simulator # Compared to prior simulation platforms, here we highlight several key features of Genesis: üêç Pythonic and fully transparent. Genesis is developed and fully open-source in python, making code understanding and contribution way more easier. üë∂ Effortless installation and extremely simple and user-friendly API design. üöÄ Parallelized simulation with unprecedented speed : Genesis is the world‚Äôs fastest physics engine , delivering simulation speeds up to 10~80x (yes, this is a bit sci-fi) faster than existing GPU-accelerated robotic simulators (Isaac Gym/Sim/Lab, Mujoco MJX, etc), without any compromise on simulation accuracy and fidelity. üí• A unified framework that supports various state-of-the-art physics solvers, modeling a vast range of materials and physical phenomena. üì∏ Photo-realistic ray-tracing rendering with optimized performance. üìê Differentiability : Genesis is designed to be fully compatible with differentiable simulation. Currently, our MPM solver and Tool Solver are differentiable, and differentiability for other solvers will be added soon (starting with rigid-body simulation). ‚òùüèª Physically-accurate and differentiable tactile sensor . üåå Native support for Generative Simulation , allowing language-prompted data generation of various modalities: interactive scenes , task proposals , rewards , assets , character motions , policies , trajectories , camera motions , (physically-accurate) videos , and more.